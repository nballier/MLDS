# BACKGROUND ON ANNOTATION AND METRICS


##BACKGROUND FOR Vajjala et al.



###POS-tagging > Constituency parsing > dependency parsing
<https://corenlp.run/>

###CoNLL-X format : 
Conference on Computational Natural Language Learning (CoNLL)

###CoNLL-U format: 
<https://universaldependencies.org/format.html>

###UD-based analysis
<http://ufal.mff.cuni.cz/udpipe>

### Universal dependencies 
<https://universaldependencies.org/format.html>

### Universal dependencies for English : 3 models
<https://github.com/UniversalDependencies/docs/blob/pages-source/_en/introduction.md>

### English web Treebank
<https://github.com/UniversalDependencies/docs/blob/pages-source/_en/introduction.md>

### Training your own model (Twitter data??)

### Applying UD to (learners of) Italian 
<https://github.com/nishkalavallabhi/UniversalCEFRScoring/blob/master/Datasets/IT-Parsed/1325_1001008_IT_B1.txt.parsed.txt>


## SOME COMPLEXITY METRICS :


### syntactic complexity : TAASC
<https://www.linguisticanalysistools.org/taassc.html>

### lexical sophistication : TAALES
<https://www.linguisticanalysistools.org/taales.html>

### DRILL: 
Here is a dataset used to evaluate Gender Bias in Machine Translation.
Evaluating Gender Bias in Machine Translation Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer, (ACL 2019), 
RQ: Research question : are the sentences really comparable in sophistication and complexity ?
<https://github.com/gabrielStanovsky/mt_gender/blob/master/translations/google/en-fr.txt>







